---
title: "Linear Regression Model"
output: html_document
---

# 線性回歸
## 簡單線性迴歸
### 前置作業
1. data 切割固定
```{r}
library(caTools)
set.seed(123)
```

2. 將mtcars以2:8切割，分別為預測的data，和訓練的data。
```{r}
split <- sample.split(mtcars, SplitRatio = 0.8)
train.df <- subset(mtcars, split == T)
predict.df <- subset(mtcars, split == F)
```

```{r}
mtcars
```



### 跑回歸
```{r}
reg <- lm(formula = hp ~ disp, data = train.df)
reg
```

```{r}
summary(reg)
```

```{r}
anova(reg)
```

```{r}
reg$coefficients
```


### 畫圖看回歸結果
```{r}
library(ggplot2)
ggplot(train.df, aes(x = disp, y = hp)) +
  geom_smooth(method = 'lm') +
  geom_point() +
  annotate("text", x = 200, y = 200,
           label = paste0("Y=",round(reg$coefficients[1],2),
                          "+", round(reg$coefficients[2],2),"X")) +
  ggtitle(label = "Simple Linear Regression")
```


## 多變量線性迴歸(Multiple Linear Regression)
### cyl(汽缸數量)、disp(排氣量)、wt(噸位)、vs(引擎種類，0=自動、1=手動)、am(變數箱，0=自動、1=手動)
```{r}
reg.mult <- lm(hp ~ cyl + disp + wt + vs + am, data = train.df)
reg.mult
```

```{r}
summary(reg.mult)
```

```{r}
anova(reg.mult)
```

### all columns
```{r}
reg.all <- lm(hp ~ ., data = train.df)
summary(reg.all)
```

```{r}
anova(reg.all)
```


## 多項式迴歸 Polynomial Regression
1. 設定變數
```{r}
mtcars2 <- mtcars
mtcars2$disp2 <- mtcars2$disp^2  # disp平方
mtcars2$disp3 <- mtcars2$disp^3  # disp立方
mtcars
mtcars2
```

2. 拆data: mtcars2
```{r}
train.df2 <- subset(mtcars2, split == T)
predict.df2 <- subset(mtcars2, split == F)
```

3. regression
```{r}
reg.pn <- lm(hp ~ disp + disp2 + disp3, data = train.df2)
reg.pn
```

```{r}
summary(reg.pn)
```

```{r}
anova(reg.pn)
```

4. picture
```{r}
ggplot(train.df2, aes(x = disp, y = hp)) +
  geom_point() +
  geom_line(aes(y = predict(reg.pn, newdata = train.df2)), colour = "blue") +
  annotate("text", x = 156, y = 200,
           label = paste0("Y = ", round(reg.pn$coefficients[1],2),round(reg.pn$coefficients[2],2), "X","+", round(reg.pn$coefficients[3],2), "X^2",
             "+", round(reg.pn$coefficients[4],2),"X^3")) +
  ggtitle(label = "Polynomial Regression")

```

5. 預測
```{r}
predict.df2$predict = predict(reg.pn, newdata = predict.df2)
predict.df2$diff = predict.df2$predict - predict.df2$hp
predict.df2
```

## 標準化回歸 Data Scaling + Mutiple Regression
1. 資料標準化
```{r}
# 原始資料
summary(mtcars)

# 標準化
mtcars.scale <- scale(mtcars)
summary(mtcars.scale)
```

```{r}
class(mtcars.scale)
as.data.frame(mtcars.scale)
```

2. 標準化回歸
```{r}
reg.scale <- lm(hp ~ ., data = as.data.frame(mtcars.scale))
reg.scale
```

```{r}
summary(reg.scale)
```

```{r}
anova(reg.scale)
```

4. 比較標準化前後回歸係數差異
```{r}
reg.all$coefficients
print(" ")
reg.scale$coefficients
```


## 羅吉斯迴歸 Logistic Regression 
```{r}
greg.s <- glm(vs ~ ., data = train.df2, family = 'binomial')
greg.s
```

```{r}
summary(greg.s)
```

```{r}
predict.df2$g.predict = ifelse(predict(greg.s, newdata = predict.df2) < 0, 0, 1)
predict.df2$vs_predic = (predict.df2$vs == predict.df2$g.predict)
predict.df2
```

```{r}
iris
```


## 練習
1. 
```{r}
reg.iris <- lm(Petal.Length ~ ., data = iris)
summary(reg.iris)
```

2. 一樣使用iris，但取出Species符合 setosa、versicolor的資料$^1$。將資料2:8拆開，利用 Sepal.Length, Sepal.Width, Petal.Width, Petal.Length 來判斷該花屬於哪種品種。

```{r}
iris2 <- subset(iris, Species != "virginica")
iris2$Species.type <- ifelse(iris2$Species == "setosa", 0, 1)
```

  1.分開資料
```{r}
set.seed(123)
split2 <- sample.split(iris2, SplitRatio = 0.8)
train.ir2 <- subset(iris2, split2 == T)
predict.ir2 <- subset(iris2, split2 == F)
```

  2. 使用train.ir2訓練logistic模型
```{r}
greg.ir2 <- glm(Species.type ~ ., data = train.ir2, family = "quasibinomial")
summary(greg.ir2)
```

  3. 使用預測資料來和訓練好的模型來預測Species。
```{r}
predict.ir2$Predict.Species.type <- predict(greg.ir2, newdata = predict.ir2)
predict.ir2$Predict.Species <- ifelse(predict.ir2$Predict.Species.type < 0, "setosa", "versicolor")

mean(predict.ir2$Species == predict.ir2$Predict.Species)
#predict.ir2
```
  
  